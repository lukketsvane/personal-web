---
title: "Knowledge Graphs and LLMs"
description: "Exploring the intersection of knowledge graphs and language models"
date: "2024-05-22"
tags: ["knowledge graphs", "LLMs", "AI", "machine learning", "data structures"]
type: "writing"
category: "Artificial Intelligence"
image: "/images/knowledge-graph-llms.png"
---

# Knowledge Graphs and LLMs: Exploring the Intersection

<img 
  src="/images/knowledge-graph-llms.png" 
  alt="Knowledge Graphs and LLMs" 
  className="w-full h-auto mb-5" 
/>

## PredictiveChat: A Novel Approach to Minimizing Latency in Conversational AI through Anticipation

<img 
  src="/images/predictive-chat-overview.png" 
  alt="Predictive Chat Overview" 
  className="w-full h-auto mb-5" 
/>

PredictiveChat introduces an innovative approach to leveraging large language models (LLMs) to anticipate user input, enabling instantaneous response generation in conversational AI systems. By predicting user messages and pre-generating responses, PredictiveChat aims to significantly enhance user experience through reduced response times.

### Flow of Data in PredictiveChat

The image above illustrates the flow of data for PredictiveChat, a proof of concept chat app which minimizes latency by predicting user messages and pre-generating responses. These responses are stored and used if the final message matches the predicted message.

*Note: This "paper" was generated by ChatGPT based on the code, author prompt, and X/Twitter discussions.*

## Abstract

PredictiveChat introduces an innovative approach to leveraging large language models (LLMs) to anticipate user input, enabling instantaneous response generation in conversational AI systems. By predicting user messages and pre-generating responses, PredictiveChat aims to significantly enhance user experience through reduced response times. This paper outlines the system's methodology, contrasts it with existing technologies, discusses its implementation, and explores potential future applications.

[GitHub](https://github.com/yoheinakajima/predictivechat) | [Original X/Twitter Thread](https://twitter.com/yoheinakajima/status/1762313302863200658)

## 1. Introduction

Conversational AI interfaces often suffer from latency, which can detract from the naturalness of human-computer interaction. PredictiveChat addresses this challenge by emulating the human ability to predict and formulate responses during conversation, aiming to achieve "negative latency" in certain instances and almost zero latency in others. This system not only seeks to reduce wait times but also leverages the additional processing time for more thoughtful and contextually relevant responses.

<img 
  src="/images/typing.gif" 
  alt="Typing Example" 
  className="w-full h-auto mb-5" 
/>

Example input: "When are the next Olympics being held?" cuts off the user message and answers correctly.

## 2. Background and Related Work

### 2.1 Predictive Technologies in Conversational AI

Existing technologies like Google's Smart Reply and Apple's QuickType have introduced predictive text and response capabilities, but they primarily focus on static predictions. In contrast, PredictiveChat dynamically anticipates the user's complete input in real-time, offering a significant advancement over these earlier approaches.

### 2.2 Innovations in Latency Reduction

Twitter discussions highlighted several theoretical approaches and practical implementations relevant to PredictiveChat's goals:

- **Vector Store and Violation of Expectation:** Inspired by @BloomBotAI, PredictiveChat utilizes vector stores as a cache mechanism, leveraging the psychological principle of "violation of expectation" to reduce latency.
- **Kalman Filter Analogies:** The system's predictive mechanism draws comparisons to the Kalman filter, handling uncertainty and noise in real-time user input prediction.
- **Chain of Thought Processing:** PredictiveChat plans to use extra compute time for deeper processing, such as Chain-of-Thought or Retrieval-Augmented Generation (RAG), enhancing AI interaction models.

## 3. System Architecture and Implementation

PredictiveChat is built on a Flask web application framework and integrates with OpenAI's GPT models for input prediction and response generation. Its architecture features three key components:

- **Input Prediction Module:** Utilizes partial user inputs to predict the remainder of the user's intended message, leveraging a custom adaptation of GPT-3.5 for dynamic prediction.
- **Response Generation Module:** Generates potential responses based on both partial and predicted inputs, facilitating instantaneous or near-instantaneous replies once the user completes their message.
- **Caching and Retrieval System:** Employs a vector-based mechanism to store message and response embeddings, allowing for rapid retrieval of similar past interactions to further reduce response times.

## 4. Data Flow and Processing

PredictiveChat's innovative data flow begins with the user's initial input, which is used to predict the message's completion and generate a response preemptively. This approach has demonstrated the potential for "negative latency" in cases where the system accurately anticipates the user's complete message, as well as almost zero latency when utilizing cached responses.

## 5. Evaluation and Insights from X/Twitter Discussion

While formal evaluation metrics and user studies are pending, initial observations suggest that PredictiveChat can achieve significant reductions in response latency. The Twitter discussion provided valuable insights into practical and theoretical innovations, highlighting the system's potential to revolutionize conversational AI with features like:

- **Speculative Decoding:** Speeding up inference in LLMs by decoding future inputs preemptively.
- **Filler Words and Stalling Phrases:** Enhancing naturalness in AI interactions by incorporating filler words during processing times.
- **Speech-to-Speech Models:** Expanding PredictiveChat to direct speech-to-speech interactions, eliminating the need for text intermediaries.

## 6. Future Directions

PredictiveChat's future opportunities include integrating Chain-of-Thought processing and/or RAG in a chat context, and exploring the potential for preemptive actions beyond chat, such as in user need prediction and robotics. The pattern could also investigate multimodal predictions to enrich interactions across different sensory inputs.

## 7. Conclusion

PredictiveChat offers a novel solution to the longstanding issue of latency in conversational AI. By predicting user inputs and leveraging advanced LLMs, the system promises a significant leap towards more natural and efficient human-computer interactions. Future enhancements, informed by ongoing discussions and technological advancements, will continue to refine and expand PredictiveChat's capabilities.